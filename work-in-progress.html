<ul>
<li><strong>Minimal models and explanations</strong></li>
</ul>
<p>One important strand in the literature on scientific modelling has focused on the issue of clarifying the precise structure and implications of the process of applying mathematics to empirical problems. It has been argued that mathematical models used in scientific practice play a series of epistemic roles, such as representing, predicting, and explaining various features of the empirical phenomena of interest. This paper contributes to the debate about the explanatory role that mathematical concepts and operations play in certain empirical contexts. Perhaps the most popular idea about how mathematics contributes to the construction of empirically adequate explanations in science is captured by so-called mapping (or structuralist) accounts of explanation, according to which mathematical models successfully explain some feature(s) of an empirical phenomenon in virtue of an appropriate mapping relation that obtains between a mathematical structure and the target physical (empirical) structure (e.g., Pincock 2007; Bueno &amp; Colyvan 2011). However, the systematic problems that threaten most mapping accounts (e.g., the assumed structure problem, the difficulty of distinguishing the empirical and the mathematical domain, the challenge of accounting for the role of idealizations in mathematical modelling and explanation) motivate the search for a new kind of approach that would deal with the cases of mathematical modelling that are not satisfactorily analyzed with the conceptual tools provided by mapping accounts. In this paper, I discuss and refine the minimal model account of explanation defended in various forms by Robert Batterman (e.g., Batterman 2002, 2009, 2010; Batterman and Rice 2014). The account focuses on a particular class of scientific explanations whose explananda are robust or repeatable patterns observed in empirical phenomena. Rather than focusing exclusively on the relation of representation, the minimal model account of explanation provides a more dynamic perspective on the construction and validation of mathematical models of empirical phenomena, emphasizing the interaction between the physical sciences and mathematics. I claim that the minimal model framework is better equipped conceptually to account for the role that idealizations play in the construction of empirically adequate explanations. Moreover, I show that the minimal model account of explanation is grounded in a pragmatist picture of scientific modelling which is consistent with the proposal that mapping accounts are also able to capture some of the modelling and explanatory strategies used in science. I conclude by considering the issue of the scope of the minimal model account of explanation.</p>
<ul>
<li><strong>How to solve the triviality problem of computational implementation</strong></li>
</ul>
<p>The notion of computational implementation is used by philosophers as well as cognitive scientists who defend a broadly computationalist view of cognition and its neural basis. While some treat the notion as a theoretical primitive or as a stand-in for any plausible account of how the neural machinery of the brain might realize the cognitive functions and behaviors observed in living beings, others have pressed for a more systematic characterization of the notion that would place computational theories of cognition on safer empirical grounds (Chalmers 1993/2012; Sieg 2002; Sprevak 2012). The litmus test for the latter type of approach has been solving the so-called triviality problem. Made famous by Searle (1990) and Putnam (1988), the triviality problem is a skeptical argument according to which any sufficiently complex physical object computes any computable function. More surprisingly perhaps, none of the proposed solutions seems to get at the heart of the triviality problem. For this reason, they have been deemed as incomplete accounts of computational implementation. In this paper, I argue that the difficulty of dealing with the Searle-Putnam-style of skeptical arguments stems from one central assumption that they share with the standard view of computational implementation. According to this assumption, the empirical adequacy of computational theories of cognition depends on the presence of an adequate structural mapping between the proposed neurobiological implementational basis and the computational description of the target cognitive function. The pragmatist view defended in the paper reveals that the structuralist mapping assumption is not enough to construct an adequate account of the notion of computational implementation. What is required is an investigation of the prior methodological and pragmatic assumptions which need to be in place for the application of the structural mapping requirement.</p>
<ul>
<li><strong>Is there a right way to do philosophy of science?</strong> (with Sara Green)</li>
</ul>
<p>A recent trend in philosophy of science has emphasized the importance of scientific practice to philosophical analysis. The thrust of this emerging tradition is that the careful survey of the experimental, modeling, and theorizing activities of scientists will shed a new light on traditional philosophical debates and open up for exploration a series of interesting new topics. The commitment of philosophy of science in practice (PSP) to provide accurate and insightful descriptive accounts of different aspects of scientific practice also implies a methodological shift in the philosophical practice itself. A number of authors have argued that this is a welcome reorientation of the field which thereby gains a new source of warrant. Philosophical analyses, it has been claimed, are susceptible to new types of empirical evidence and revisable in their light. In response, critics have pointed out that this methodological move undermines the normative role of philosophy which becomes a mere commentator of the achievements of scientific research. The twin challenge raised in connection to the methodological commitments of PSP theorists is: (i) to provide a convincing <em>motivation</em> for pursuing descriptive accounts of scientific practice, (ii) to show how these descriptive projects relate to the intrinsic normative dimension of philosophical analysis. The latter challenge amounts to showing how the <em>methodological</em> commitments of PSP enhance particular philosophical accounts.</p>
<p>We contend that the strength of this twofold challenge is primarily due to a misunderstanding of the relationship between the descriptive and normative dimensions of PSP projects. The strategy we propose in this paper aims to clarify the implications of the methodological commitments of PSP research and dispel some of the sources of misunderstanding concerning the normative ambitions of philosophical accounts developed in the PSP tradition. For this purpose, we begin by analyzing the debate between two distinct projects concerning the <em>gene</em> concept. We identify the methodological differences between these two projects and show how they relate to the proposed philosophical accounts. The more general question raised by these explorations concerns the warrants and sources of evidence which are taken to support particular philosophical analyses. We argue that clarifying the different types of evidence relevant for these two philosophical projects affords also a better understanding of the differences between the aims pursued by their proponents. In section 3 we discuss the motivational and methodological challenges raised by Brunnander (2011) in the context of the two accounts analyzed in section 2. We propose a strategy for dealing with these challenges that would strengthen and further differentiate these accounts. Section 4 develops this proposal and shows that it sheds light on the intricate relationship between the aims associated with different philosophical projects and the methodologies and sources of evidence they rely on. Then, in section 5 we return to the general question concerning the relationship between the descriptive and normative dimensions of philosophical analysis arguing for a pluralist solution to the puzzles raised by their interaction.</p>
<ul>
<li><strong>Network approaches in systems biology: something new and something borrowed</strong> (with Sara Green, William Bechtel, and Raphael Scholl)</li>
</ul>
<p>The emergence of systems biology is linked to the limitations of research strategies used in molecular biology to investigate and explain organizational and quantitative aspects of large systems of biological interactions. In consequence, a lot of research in systems biology focuses on quantitative and relational aspects of biological processes, complementing research strategies in molecular biology with new tools that can handle much larger datasets. In this paper we investigate the contributions of network approaches to various projects within systems biology. We identify the main motivations for applying the tools of network analysis to studying the organization and dynamics of biological systems. In addition to highlighting some shared characteristics of network approaches, we point out important differences in the research methodology, research questions and assumptions underlying different types of analysis of biological networks. Among these, we focus on graph theoretical approaches which categorize biological networks into functional types by identifying their organizational features (clustering coefficients, modularity, hierarchical organization etc.), and global approaches to systems dynamics which draw on dynamical systems theory. We argue that the differences between these approaches have important implications for how we think about living systems. We also consider when these two types of network approaches used in systems biology come in conflict with each other and what is required for a better interaction between them.</p>
<ul>
<li><strong>Neural network simulations: between theoretical modelling and experimentation</strong></li>
</ul>
<p>The task of explaining how various brain structures achieve the complex cognitive functions and behaviors observed in living organisms faces the major challenge of bridging the gap between higher-level or abstract descriptions of psychological capacities and behaviors and lower-level accounts of the structure and organization of neurobiological systems. The <em>Human Brain Project</em>, successor of the <em>Blue Brain Project</em> (Markram 2004) promises to create a framework which allows for the integration of experimental data and theoretical hypotheses targeting different levels of organization of biological organisms and their psychological functions. One means by which the project promises to achieve this is by generating powerful simulations of the mouse and human brain. But how can simulations facilitate our understanding of the mechanisms underlying various cognitive functions like spatial perception, face recognition, reading, or language learning, among others? What are the warrants that inferences drawn from large-scale simulations of the mouse brain will carry over to the human brain and behavior?</p>
<p>Advocates of large scale simulations point to three main epistemic advantages of this methodology. First, computational simulations increasingly make possible the integration of models developed at different spatial and temporal scales thus producing a type of <em>synthetic</em> knowledge which might be critical to understanding psychological phenomena and their neurobiological underpinnings. Second, because computational simulations allow the formalization and testing of multiscale cognitive models, they may be used to explore the limits of current theoretical proposals that cannot be directly assessed in an experimental setting. Third, because simulations can be made to switch dynamically between different levels of description, they are likely to help experimentalists and theoreticians to choose the appropriate level of detail for asking new questions and exploring new hypotheses about the cognitive architecture of the brain and the neural realizers of different cognitive functions. These epistemological advantages have been challenged on the grounds that simulations make the relations between different levels of biological organization epistemically opaque. In addition, simulations are criticized for occluding the lack of proper empirical support for certain theoretical models used in cognitive neuroscientific research. Neuroscientists participating in the <em>Brain Initiative</em> emphasize the need to develop better technologies for collecting more data about the neuronal structures underlying different cognitive functions. They claim that only in light of a complete experimental knowledge we can hope to provide an empirically adequate explanation of the observed psychological patterns and behaviors.</p>
<p>Taking into account the many methodological challenges facing large scale simulations, I propose a way to refine the epistemic roles they may in principle play in cognitive and neuroscientific research. Drawing on a class of models used in spatial perception, I argue that computer simulations play a mediating or bridging role between theoretical modelling and experimentation in cognitive neuroscience.</p>
<ul>
<li><strong>Simple mathematical models for complex causal processes: the case of reaction-diffusion models</strong></li>
</ul>
<p>What is the role of mathematical modeling in the investigation and explanation of empirical biological phenomena? Various theorists have argued that the idealizing operations involved in mathematical modeling enable essential causal inferences about the behaviors of target systems that would otherwise be occluded. More generally, mathematical models can be taken to generate novel questions that biologists can further explore. One characteristic feature of mathematical models is that they can show why certain stable features or patterns are observed in systems with wildly different constitutions. Nevertheless, the use of highly simplified mathematical models in the biological sciences has not remained unchallenged. One influential idea is that the validation of theoretical (mathematical) models requires that they be incorporated within well-established experimental studies. Assuming such a uniformity of model validation, what are the strategies used to achieve this type of integration? What can scientists learn from the failures of this type of integration?</p>
<p>The present paper explores these questions in the context of analyzing Turing’s reaction­-diffusion model of pattern formation in the developing animal embryo. There are two primary motivations for choosing this model. First, Turing’s (1952) The Chemical Basis of Morphogenesis distinctively illustrates a certain style of modeling in which something like the complex morphology of living things is to be explained by appeal to a series of strong idealizing or simplifying assumptions and mathematical theorem proving. Second, I contend that there are several important lessons to be drawn from the history of the development of reaction diffusion models in developmental biology and their reception by experimental biologists.</p>
<p>Turing selected chemical reaction and diffusion as the basis of his study, and analyzed the behavior of a hypothetical system (reaction­-diffusion system) composed of two kinds of diffusible and interacting chemicals. The key result of reaction-­diffusion models is that two homogeneously distributed substances within a certain space, one of which is locally activated and the other capable of long­ range inhibition can produce novel shapes and gradients. The model shows that the results of these substances interactions are dependent only on four variables per morphogen ­ the rate of production, the rate of degradation, the rate of diffusion, and the strength of their activating/inhibiting interactions. The most striking feature of this mathematical model is that it can be taken to explain pattern formation without a preformed pattern, or, otherwise put, it can explain how patterns in different systems arise in the first place.</p>
<p>Despite its simplicity, the biological relevance of the Turing model has been challenged in light of the experimental discoveries about early embryonic development in <em>Drosophila melanogaster</em>. Subsequent studies have argued that fly patterning is much more complicated than a Turing model would suggest and that the segment patterns are determined by more than two morphogenes. However, the emerging skepticism towards reaction-­diffusion models has been somewhat counteracted by other more successful applications to the explanation of patterns such as hair follicle distribution in mice, stripe formation in zebrafish and left­-right asymmetry in bilaterians (cf. Kondo &amp; Miura 2010). What are the important differences between these applications of the Turing model? What seems to guarantee the successful application of mathematical models in some cases but not in others?</p>
<p>The development and reception of Turing (reaction- ­diffusion) models raises a series of important questions about the epistemic roles of highly simplified mathematical models of biological phenomena or patterns. On the one hand, these models capture important dependency relations between different variables that characterize the properties and/behaviors of biological organisms. However, scientists seem to be reluctant to infer directly from these dependency relations qualitative causal accounts of the observed developmental phenomena. One recent proposal is that computer simulation studies can afford valuable insight into how the Turing model can generate certain types of patterns observed across a wide range of biological systems. In the final part of the paper I explore the ways in which simulation studies contribute to improving causal accounts of the kind of features targeted by reaction-­diffusion models.</p>
